# Конспект лекции: Методы искусственного интеллекта

## Введение и цели курса

**Лектор:** Карпов Егор Константинович

**Ключевые цели дисциплины:**
Основная задача курса — не просто изучить теорию искусственного интеллекта, а научиться применять его как мощный инструмент для анализа данных в рамках практической, проектной деятельности. Студенты должны будут научиться:
*   **Адекватно применять математические методы:** Понимать, какой метод подходит для конкретной задачи, и уметь обосновать свой выбор.
*   **Проводить предварительный анализ данных:** Углубленно изучать данные, находить в них закономерности и аномалии.
*   **Создавать и оценивать модели:** Строить предиктивные и интерпретирующие модели, а также оценивать их эффективность не только с математической, но и с бизнес-точки зрения.
*   **Представлять результаты:** Уметь донести суть и пользу своего проекта до заказчиков, которые могут не разбираться в математике, но должны понимать ценность продукта.

**Рекомендуемые инструменты:**
*   **Jupyter Notebook:** Основной инструмент, который будет использоваться на практических занятиях. Лектор рекомендует установить его.
*   **Google Colab:** Облачная альтернатива, которую также можно использовать. Однако лектор предупреждает, что могут возникнуть небольшие различия в коде или установке библиотек.

---

## 1. Краткая история и ключевые этапы развития ИИ

История искусственного интеллекта — это путь от простых логических систем до сложных нейронных сетей, способных имитировать человеческое общение.

**Этапы развития:**
1.  **Ранний ИИ (с 1950-х): Попытка имитации мозга.**
    *   **Пример лектора:** В 1952 году была создана первая модель, которая являлась программно-аппаратной имитацией нейронов человеческого мозга. Это устройство состояло из нескольких "нейронов" и было способно выполнять простую задачу: распознавать базовые геометрические фигуры (круги, квадраты, треугольники).
2.  **Машинное обучение (Machine Learning, 1980-е - 2010-е): Статистический подход.**
    *   В этот период акцент сместился на разработку алгоритмов, которые обучаются на основе статистических закономерностей в данных. Сюда входят экспертные системы и классические методы машинного обучения.
3.  **Глубокое обучение (Deep Learning, с 2010-х): Эпоха многослойных нейросетей.**
    *   Современный этап, характеризующийся появлением очень сложных, "глубоких" (многослойных) нейронных сетей. Благодаря росту вычислительных мощностей и появлению новых алгоритмов обучения, стали возможны такие прорывы, как большие языковые модели (LLM), которые демонстрируют впечатляющие способности в имитации человеческой речи и генерации контента.

### Глубокое обучение vs. Машинное обучение: Ключевое различие

Лектор сделал особый акцент на фундаментальном различии между этими двумя подходами.

*   **Машинное обучение (ML): Обучение с учителем и ручное извлечение признаков.**
    *   В классическом ML процесс обучения требует значительного участия человека, который выступает в роли "учителя".
    *   **Пример лектора с котиками:** Чтобы научить ML-модель распознавать фотографии с котиками, нужно:
        1.  Собрать огромный датасет (например, 100 000 фотографий).
        2.  Нанять людей (или использовать сервисы вроде "Яндекс.Толока"), которые вручную просмотрят каждую фотографию и поставят метку: "кот" или "не кот".
        3.  Этот процесс называется **извлечением признаков (feature extraction)** и **разметкой данных**. Человек сам решает, какие признаки важны.
    *   Трудности: Этот подход очень трудоемкий, дорогой и требует качественной работы разметчиков.

*   **Глубокое обучение (DL): Автоматическое извлечение признаков.**
    *   Главное преимущество DL в том, что нейросеть способна **самостоятельно** извлекать признаки из сырых данных.
    *   **Пример лектора с котиками (продолжение):** В случае с DL мы можем просто "загрузить" в нейросеть те же 100 000 фотографий, и она сама, слой за слоем, научится распознавать сначала простые признаки (грани, углы), затем более сложные (уши, усы, глаза) и, наконец, сможет с высокой точностью определять котиков.
    *   Это позволяет автоматизировать самую сложную часть работы и открывает возможности для обучения на неразмеченных данных (обучение без учителя).

---

## 2. Ключевые задачи машинного обучения

Машинное обучение применяется для решения широкого спектра задач.

1.  **Классификация:** Присвоение объекту определенной категории.
    *   **Примеры лектора:**
        *   Определить, кто на фото: кошка или собака.
        *   Распознать, разговаривает ли водитель по телефону во время движения автомобиля.
        *   Определить, переходит ли пешеход дорогу в положенном месте.

2.  **Регрессия:** Предсказание непрерывного числового значения.
    *   **Пример лектора:** Предсказать рыночную стоимость квартиры, основываясь на ее площади, районе, количестве комнат и т.д.

3.  **Преобразование последовательности (Sequence-to-Sequence):** Трансформация одной последовательности в другую.
    *   **Примеры лектора:**
        *   **Машинный перевод:** последовательность слов на одном языке преобразуется в последовательность на другом.
        *   **Распознавание речи:** последовательность звуков преобразуется в последовательность букв (текст).

4.  **Кластеризация:** Группировка данных на основе их схожести без предварительной разметки.
    *   **Пример лектора:** Представьте датасет на 50 миллионов строк и 50 тысяч столбцов. Человек физически не способен проанализировать такие объемы. Нейросеть же может найти в этих данных скрытые группы (кластеры) и закономерности, которые не видны человеку.

5.  **Понижение размерности:** Уменьшение количества признаков (столбцов) для упрощения модели и ускорения обучения.
    *   **Пример лектора:** В университете есть данные о 30 000 студентах, и для каждого — 3000 признаков (от ФИО и почты до количества заходов на страницу каждого задания). Многие из этих признаков взаимозависимы. Методы понижения размерности позволяют автоматически найти и объединить зависимые признаки, оставив только наиболее информативные и независимые компоненты.

---

## 3. Детальный процесс машинного обучения

Лектор подчеркнул важность принципа **"Мусор на входе — мусор на выходе"**: качество конечной модели напрямую зависит от качества данных.

1.  **Сбор данных:**
    *   **Что это:** Планирование и сбор информации из различных источников (анкеты, датчики, базы данных, API).
    *   **Дополнительная информация:** На этом этапе важно сформулировать гипотезу: какие данные могут повлиять на результат? Иногда нужно собирать данные "про запас", потому что потом получить их будет невозможно.

2.  **Подготовка данных:**
    *   **Что это:** Очистка и приведение данных к пригодному для анализа виду.
    *   **Примеры лектора:**
        *   **Обработка анкет:** Студентам дали задание написать в поле "да" или "нет". В результате были получены десятки вариантов: "да", "Да", "ДА", "дА", "yes", "true", "1", "не" вместо "нет", и даже "слон". Все это нужно привести к единому формату (`1` или `0`), а некорректные строки (как со "слоном") — удалить.
        *   **Поиск логических ошибок:** В данных о продаже квартир можно найти объекты с высотой потолков 100 метров или 1 метр. В данных о температуре воды — значение 700°C. Это логические ошибки, которые нужно исправлять или удалять.

3.  **Обучение модели:**
    *   **Что это:** Выбор алгоритма, передача ему подготовленных данных и процесс "тренировки".
    *   **Дополнительная информация:** На этом этапе данные обычно делят на три части: **обучающую** (для тренировки модели), **валидационную** (для тонкой настройки в процессе) и **тестовую** (для финальной, независимой оценки качества).

4.  **Анализ и оценка модели:**
    *   **Что это:** Проверка метрик качества модели (точность, полнота и т.д.) и ее соответствия бизнес-целям.
    *   **Дополнительная информация:** Модель может иметь высокую математическую точность (99%), но быть бесполезной для бизнеса. Например, модель предсказывает клики по рекламе с точностью 99%, но делает это, всегда предсказывая "не кликнет" (так как кликов в реальности очень мало).

5.  **Внедрение (Deployment):**
    *   **Что это:** Интеграция обученной модели в реальное приложение или бизнес-процесс.
    *   **Дополнительная информация:** Модель может быть встроена в веб-сайт, мобильное приложение или внутреннюю аналитическую систему.

6.  **Переобучение и дообучение:**
    *   **Что это:** Модели со временем устаревают, так как данные в реальном мире меняются. Их нужно периодически обновлять.
    *   **Пример лектора:** Камера контроля скорости неправильно распознала пробку и выписала штрафы всем стоящим машинам. Разработчики проанализировали эту ошибку, добавили похожие ситуации в обучающий датасет и **дообучили** модель, чтобы она больше не совершала таких ошибок.

---

## 4. Стандартный процесс CRISP-DM

**CRISP-DM** — это методология, которая структурирует работу над проектом по анализу данных.

1.  **Понимание бизнеса (10% времени):** Определить цели. *Пример: не "построить модель", а "увеличить продажи на 15% за счет персонализированных рекомендаций".*
2.  **Понимание данных (20% времени):** Собрать и изучить данные. Есть ли они вообще? Достаточно ли их?
3.  **Подготовка данных (45% времени):** **Самый долгий этап.** Очистка, объединение из разных систем, создание новых признаков (feature engineering).
4.  **Построение модели (10% времени):** Обучение нескольких моделей и выбор лучшей.
5.  **Оценка (10% времени):** Проверка, достигнуты ли бизнес-цели, поставленные на первом этапе.
6.  **Внедрение (5% времени):** Упаковка решения и его интеграция. Разработка плана поддержки.

---

## 5. Математический фундамент

### Важность визуализации: Квартет Энскомба

Лектор продемонстрировал **квартет Энскомба**: четыре графика, которые выглядят совершенно по-разному, но имеют **абсолютно одинаковые** статистические показатели: среднее значение, дисперсию, корреляцию и уравнение регрессии.

*   **Вывод:** Числам и коэффициентам нельзя слепо доверять. **Всегда визуализируйте свои данные.** График может показать выбросы, нелинейные зависимости или скрытые кластеры, которые не видны в сухих цифрах.

### Выборки и смещения

*   **Генеральная совокупность vs. Выборка:** Мы почти никогда не имеем доступа ко всем данным (генеральной совокупности), поэтому работаем с их частью (выборкой).
*   **Репрезентативность:** Выборка должна правильно отражать структуру всей совокупности.
*   **Смещение выборки (Bias):** Систематическая ошибка, которая ведет к неверным выводам.
    *   **Пример лектора:** Если вы хотите обучить нейросеть для торговли на фондовом рынке и будете показывать ей только примеры, где акции росли, она научится всегда предсказывать рост. В реальных условиях такая модель приведет к катастрофе.
    *   **Другой пример:** Опрос о популярности напитка, проведенный только среди студентов, не будет отражать мнение людей всех возрастов.

---

## 6. Решение домашнего задания

На лекции была предложена следующая задача:

**Дано:**
*   Две булевы случайные величины A и B.
*   Вероятность P(A) = 1/2.
*   Вероятность P(B) = 1/3.
*   Условная вероятность P(A | не-B) = 1/4.

**Найти:**
*   Условную вероятность P(A | B).

### Пошаговое решение:

1.  **Цель:** Нам нужно найти `P(A | B)`. Вспомним формулу условной вероятности:
    `P(A | B) = P(A ∩ B) / P(B)`
    Здесь `P(A ∩ B)` — это вероятность того, что события A и B произойдут одновременно. `P(B)` нам уже известно. Значит, наша задача — найти `P(A ∩ B)`.

2.  **Используем формулу полной вероятности.** Вероятность события A можно представить как сумму вероятностей его наступления вместе с событием B и вместе с событием "не-B":
    `P(A) = P(A ∩ B) + P(A ∩ не-B)`

3.  **Найдем `P(A ∩ не-B)`.** У нас есть `P(A | не-B)`. Распишем эту условную вероятность по формуле:
    `P(A | не-B) = P(A ∩ не-B) / P(не-B)`
    Отсюда:
    `P(A ∩ не-B) = P(A | не-B) * P(не-B)`

4.  **Вычислим `P(не-B)`.** Так как B — булева величина, то:
    `P(не-B) = 1 - P(B) = 1 - 1/3 = 2/3`

5.  **Теперь вычислим `P(A ∩ не-B)`:**
    `P(A ∩ не-B) = (1/4) * (2/3) = 2/12 = 1/6`

6.  **Найдем `P(A ∩ B)` из формулы полной вероятности (шаг 2):**
    `P(A) = P(A ∩ B) + P(A ∩ не-B)`
    `1/2 = P(A ∩ B) + 1/6`
    `P(A ∩ B) = 1/2 - 1/6 = 3/6 - 1/6 = 2/6 = 1/3`

7.  **Финал. Вычисляем искомую вероятность `P(A | B)` (шаг 1):**
    `P(A | B) = P(A ∩ B) / P(B) = (1/3) / (1/3) = 1`

**Ответ:** `P(A | B) = 1`.